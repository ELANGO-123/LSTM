# ============================================================
# Multivariate Time-Series Forecasting
# Seq2Seq LSTM with Self-Attention + Hyperparameter Optimization
# Baselines: LSTM (no attention) and SARIMA
# Metrics: RMSE, MAE, MAPE
# ============================================================

import numpy as np
import pandas as pd
import tensorflow as tf
import keras_tuner as kt

from tensorflow.keras.layers import LSTM, Dense, Input, Attention
from tensorflow.keras.models import Model, Sequential
from tensorflow.keras.callbacks import EarlyStopping

from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error

from statsmodels.tsa.statespace.sarimax import SARIMAX

# -------------------------------
# 1. DATA GENERATION
# -------------------------------
np.random.seed(42)

n_samples = 2500
time = np.arange(n_samples)

data = pd.DataFrame({
    "feature_1": np.sin(0.02 * time) + np.random.normal(0, 0.1, n_samples),
    "feature_2": np.cos(0.015 * time) + np.random.normal(0, 0.1, n_samples),
    "feature_3": np.sin(0.01 * time) * 0.5 + np.random.normal(0, 0.05, n_samples),
    "feature_4": np.random.normal(0, 1, n_samples),
    "feature_5": np.log1p(time)
})

data["target"] = (
    0.4 * data["feature_1"]
    + 0.3 * data["feature_2"]
    + 0.2 * data["feature_3"]
    + 0.1 * data["feature_5"]
    + np.random.normal(0, 0.05, n_samples)
)

# -------------------------------
# 2. PREPROCESSING
# -------------------------------
scaler = MinMaxScaler()
scaled_data = scaler.fit_transform(data)

def create_sequences(data, input_len=30, output_len=1):
    X, y = [], []
    for i in range(len(data) - input_len - output_len):
        X.append(data[i:i+input_len, :-1])
        y.append(data[i+input_len:i+input_len+output_len, -1])
    return np.array(X), np.array(y)

X, y = create_sequences(scaled_data)

train_size = int(0.7 * len(X))
val_size = int(0.85 * len(X))

X_train, y_train = X[:train_size], y[:train_size]
X_val, y_val = X[train_size:val_size], y[train_size:val_size]
X_test, y_test = X[val_size:], y[val_size:]

# -------------------------------
# 3. HYPERPARAMETER-TUNED ATTENTION MODEL
# -------------------------------
def build_attention_model(hp):
    units = hp.Choice("lstm_units", [32, 64, 128])
    lr = hp.Choice("learning_rate", [1e-2, 1e-3, 1e-4])

    inputs = Input(shape=(X_train.shape[1], X_train.shape[2]))

    encoder = LSTM(units, return_sequences=True, return_state=True)
    encoder_outputs, _, _ = encoder(inputs)

    attention = Attention()
    context = attention([encoder_outputs, encoder_outputs])

    decoder = LSTM(units)
    decoder_output = decoder(context)

    outputs = Dense(1)(decoder_output)

    model = Model(inputs, outputs)

    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=lr),
        loss="mse"
    )
    return model

tuner = kt.RandomSearch(
    build_attention_model,
    objective="val_loss",
    max_trials=6,
    executions_per_trial=1,
    directory="tuning_logs",
    project_name="lstm_attention"
)

tuner.search(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=20,
    batch_size=32,
    callbacks=[EarlyStopping(patience=3)]
)

best_model = tuner.get_best_models(1)[0]
best_hps = tuner.get_best_hyperparameters(1)[0]

print("\nBest Hyperparameters:")
print("LSTM Units:", best_hps.get("lstm_units"))
print("Learning Rate:", best_hps.get("learning_rate"))

# -------------------------------
# 4. TRAIN FINAL OPTIMIZED MODEL
# -------------------------------
best_model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=50,
    batch_size=32,
    callbacks=[EarlyStopping(patience=5, restore_best_weights=True)],
    verbose=1
)

# -------------------------------
# 5. BASELINE LSTM (NO ATTENTION)
# -------------------------------
baseline_lstm = Sequential([
    LSTM(64, input_shape=(X_train.shape[1], X_train.shape[2])),
    Dense(1)
])

baseline_lstm.compile(optimizer="adam", loss="mse")

baseline_lstm.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=50,
    batch_size=32,
    callbacks=[EarlyStopping(patience=5, restore_best_weights=True)],
    verbose=1
)

# -------------------------------
# 6. SARIMA BASELINE
# -------------------------------
sarima_model = SARIMAX(
    data["target"][:train_size],
    order=(1, 1, 1),
    seasonal_order=(1, 1, 1, 12)
)

sarima_fit = sarima_model.fit(disp=False)
sarima_preds = sarima_fit.forecast(len(y_test))

# -------------------------------
# 7. EVALUATION
# -------------------------------
def evaluate(y_true, y_pred, name):
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    mae = mean_absolute_error(y_true, y_pred)
    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100
    print(f"{name} --> RMSE: {rmse:.4f}, MAE: {mae:.4f}, MAPE: {mape:.2f}%")

attention_preds = best_model.predict(X_test)
baseline_preds = baseline_lstm.predict(X_test)

evaluate(y_test.flatten(), attention_preds.flatten(), "LSTM + Attention (Tuned)")
evaluate(y_test.flatten(), baseline_preds.flatten(), "LSTM Baseline")
evaluate(y_test.flatten(), sarima_preds[:len(y_test)], "SARIMA")

# -------------------------------
# 8. CONCLUSION
# -------------------------------
print("\nConclusion:")
print("Hyperparameter-optimized LSTM with attention outperforms")
print("both the baseline LSTM and SARIMA by dynamically focusing")
print("on the most relevant time steps in multivariate sequences.")
